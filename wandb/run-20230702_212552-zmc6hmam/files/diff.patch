diff --git a/mj_cassie.xml b/mj_cassie.xml
index 2d9f11a..a1af58c 100644
--- a/mj_cassie.xml
+++ b/mj_cassie.xml
@@ -1,7 +1,7 @@
 <mujoco model="cassie">
   <compiler eulerseq="zyx" meshdir="assets/mj_cassie" texturedir="./assets/mj_cassie" autolimits="true"/>
 
-  <option timestep="0.0005"/>
+  <option timestep="0.0005" iterations="50" solver="PGS" gravity='0 0 -9.81'/>
   <!-- Timestep is set to 0.0005 because the standard Cassie controller runs at 2 kHz -->
   <!-- Larger values still have stable dynamics -->
 
diff --git a/testing.py b/testing.py
index 938523e..fa924e6 100644
--- a/testing.py
+++ b/testing.py
@@ -9,7 +9,9 @@ from stable_baselines3.common.evaluation import evaluate_policy
 
 import gymnasium as gym
 from gymnasium.envs.registration import register
-from stable_baselines3.common.callbacks import EvalCallback
+from stable_baselines3.common.callbacks import EvalCallback, CallbackList
+import wandb
+from wandb.integration.sb3 import WandbCallback
 
 register(id='Digit-v1',
 		entry_point='digit:DigitEnv',
@@ -46,29 +48,53 @@ def visualize_reference_traj():
 				learning_rate=1e-3,)
 	evaluate_policy(model, env, render=True, n_eval_episodes=10)
 
+def train_model():
+	config = {
+		"policy_type": "MlpPolicy",
+		"total_timesteps": int(1e7),
+		"env_id": "Cassie-v1",
+		"progress_bar": True,
+		"verbose": 1,
+		"learning_rate": 3e-4,
+	}
+	run = wandb.init(
+		project="sb3",
+		config=config,
+		sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
+		monitor_gym=True,  # auto-upload the videos of agents playing the game
+		save_code=True,  # optional
+	)
+	wandbcallback = WandbCallback(
+			model_save_path=f"models/{run.id}",
+			model_save_freq=10000,
+			gradient_save_freq=10000,
+			verbose=2,
+		)
+	env = make_vec_env(config['env_id'], n_envs=4,)
+	# Separate evaluation env
+	eval_env = make_vec_env(config['env_id'], n_envs=1,)
+	# Use deterministic actions for evaluation
+	eval_callback = EvalCallback(eval_env, best_model_save_path="./logs/",
+									log_path="./logs/", eval_freq=10000,
+									deterministic=True, render=False)
+	callback_list = CallbackList([eval_callback, wandbcallback])
+	# Init model
+	model = SAC("MlpPolicy",
+				env,
+				verbose=config["verbose"],
+				learning_rate=config['learning_rate'],)
+	
+	model.learn(
+		total_timesteps=config["total_timesteps"],
+		callback=callback_list,
+		progress_bar=config["progress_bar"],
+		log_interval=100,
+	)
+	run.finish()
+
 if __name__ == "__main__":
 	train = True
 	if train:
-		# Create the environment
-		env = make_vec_env("Cassie-v1", n_envs=16, env_kwargs={'exclude_current_positions_from_observation': False})
-		# Separate evaluation env
-		eval_env = make_vec_env("Cassie-v1", n_envs=1, env_kwargs={'exclude_current_positions_from_observation': False})
-		# Use deterministic actions for evaluation
-		eval_callback = EvalCallback(eval_env, best_model_save_path="./logs/",
-									log_path="./logs/", eval_freq=1000,
-									deterministic=True, render=False)
-		# Init model
-		model = SAC("MlpPolicy",
-					env,
-					buffer_size=200000,
-					verbose=1,
-					ent_coef=0.01,
-					learning_rate=5e-3,)
-
-		# Train the agent
-		model.learn(total_timesteps=6e6,
-					log_interval=100,
-					progress_bar=True,
-					callback=eval_callback)
+		train_model()
 	else:
 		visualize_reference_traj()
\ No newline at end of file
