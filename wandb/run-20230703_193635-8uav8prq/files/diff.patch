diff --git a/cassie_viz.py b/cassie_viz.py
index 037254b..a6fc43b 100644
--- a/cassie_viz.py
+++ b/cassie_viz.py
@@ -131,14 +131,14 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
         )
 
     def step(self, action):
-        ref_mpos, ref_mvel, ref_torque = self.ref_trajectory.action(self.timestamp)
-        ref_qpos, ref_qvel = self.ref_trajectory.state(self.timestamp)
+        ref_mpos, ref_mvel, ref_torque = self.ref_trajectory.action(self.timestamp * self.frame_skip)
+        ref_qpos, ref_qvel = self.ref_trajectory.state(self.timestamp * self.frame_skip)
         
         
         xy_position_before = mass_center(self.model, self.data)
         
         zero_action = np.zeros(10)
-        self.do_simulation(zero_action, self.frame_skip)
+        self.do_simulation(zero_action, 1)
         position = self.data.qpos.flat.copy()
         rod_index = [10, 11, 12, 13, 17, 18, 19, 24, 25, 26, 27, 31, 32, 33]
         ref_qpos[rod_index] = position[rod_index]
diff --git a/mj_cassie.py b/mj_cassie.py
index 22ce339..48dadb5 100644
--- a/mj_cassie.py
+++ b/mj_cassie.py
@@ -5,8 +5,11 @@ from gymnasium import utils
 from gymnasium.spaces import Box
 import mujoco
 
+# from cassie_m.cassiemujoco import CassieSim, CassieVis
+
 from reference_trajectories.loadstep import CassieTrajectory
 
+
 DEFAULT_CAMERA_CONFIG = {
     "trackbodyid": 1,
     "distance": 4.0,
@@ -29,17 +32,15 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
             "depth_array",
         ],
         "render_fps": 67,
-        "render_fps": 400,
-        "render_fps": 67,
     }
 
     def __init__(
             self,
-            forward_reward_weight=0.1,
+            forward_reward_weight=1.25,
             ctrl_cost_weight=0.1,
             healthy_reward=5.0,
             terminate_when_unhealthy=True,
-            healthy_z_range=(0.6, 1.0),
+            healthy_z_range=(0.8, 2.0),
             reset_noise_scale=1e-3,
             exclude_current_positions_from_observation=True,
             **kwargs,
@@ -65,30 +66,45 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
                 low=-np.inf, high=np.inf, shape=(671,), dtype=np.float64
             )
         self.frame_skip = 30
-
-        self.frame_skip = 30
         MujocoEnv.__init__(
             self,
             os.getcwd()+"/scene.xml",
             self.frame_skip,
-            5,
-            self.frame_skip,
             observation_space=observation_space,
             default_camera_config=DEFAULT_CAMERA_CONFIG,
             **kwargs,
         )
 
-        self.ref_trajectory = CassieTrajectory("reference_trajectories/cassie_walk/cassie_walking.mat")
-        # print(self.ref_trajectory.time.shape)
-        # exit()
-        # print(self.ref_trajectory.time.shape)
-        # exit()
+        self.ref_trajectory = CassieTrajectory("reference_trajectories/cassie_walk/cassie_walking_from_stand.mat")
 
         self.timestamp = 0
-
-        initial_qpos, initial_qvel = self.ref_trajectory.state(0)
         
-
+        self._update_init_qpos()
+        
+        # print(f'final x pos {self.data.qpos[0]}, {self.ref_trajectory.qpos[0, 0]}')
+
+        # self.sim = CassieSim(os.getcwd()+"/cassie.xml")
+        # self.visual = True
+        # if self.visual:
+        #     self.vis = CassieVis(self.sim)
+        # self.vis.draw(self.sim)
+
+    def _update_init_qpos(self):
+        # handcrafted init qpos
+        qpos_init_cassiemujoco = np.array([0, 0, 1.01, 1, 0, 0, 0,
+            0.0045, 0, 0.4973, 0.9785, -0.0164, 0.01787, -0.2049,
+            -1.1997, 0, 1.4267, 0, -1.5244, 1.5244, -1.5968,
+            -0.0045, 0, 0.4973, 0.9786, 0.00386, -0.01524, -0.2051,
+            -1.1997, 0, 1.4267, 0, -1.5244, 1.5244, -1.5968])
+        self.init_qpos = qpos_init_cassiemujoco
+
+        # self.init_qpos = self.ref_trajectory.qpos[0]
+        # self.do_simulation(np.zeros(0), 1)
+        self.init_qpos = np.array([0.04529737116916673, -0.15300356752917388, 0.9710095501646747, 1.0, 0.0, 0.0, 0.0, 0.04516039439535328, 0.0007669903939428207, 0.48967542963286953, 0.5366660119008494, -0.5459706642036749, 0.13716932320803393, 0.6285620114754674, -1.3017744461194523, -0.03886484136807136, 1.606101853366077, -0.7079960941663008, -1.786147490968169, 0.3175519006511133, -1.683487349162938, -0.04519107401111099, -0.0007669903939428207, 0.4898192403317338, 0.38803053590372555, -0.25971548696569596, 0.49875340077344466, -0.7302227155144948, -1.3018703199186952, -0.038780951793733864, 1.606065900691361, 0.49858954295641644, -1.6206843700546072, 0.12408356187240471, -1.6835283352121144])
+        self.init_qvel = self.ref_trajectory.qvel[0]
+        # self.set_state(self.init_qpos, self.init_qvel)
+        self.reset()
+        
     @property
     def healthy_reward(self):
         return (
@@ -137,6 +153,38 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
         )
 
     def step(self, action):
+        # xy_position_before = mass_center(self.model, self.data)
+        # self.do_simulation(action, self.frame_skip)
+        # xy_position_after = mass_center(self.model, self.data)
+
+        # xy_velocity = (xy_position_after - xy_position_before) / self.dt
+        # x_velocity, y_velocity = xy_velocity
+
+        # ctrl_cost = self.control_cost(action)
+
+        # forward_reward = self._forward_reward_weight * x_velocity
+        # healthy_reward = self.healthy_reward
+
+        # rewards = forward_reward + healthy_reward
+
+        # observation = self._get_obs()
+        # reward = rewards - ctrl_cost
+        # terminated = self.terminated
+        # info = {
+        #     "reward_linvel": forward_reward,
+        #     "reward_quadctrl": -ctrl_cost,
+        #     "reward_alive": healthy_reward,
+        #     "x_position": xy_position_after[0],
+        #     "y_position": xy_position_after[1],
+        #     "distance_from_origin": np.linalg.norm(xy_position_after, ord=2),
+        #     "x_velocity": x_velocity,
+        #     "y_velocity": y_velocity,
+        #     "forward_reward": forward_reward,
+        # }
+
+        # if self.render_mode == "human":
+        #     self.render()
+        # return observation, reward, terminated, False, info
         xy_position_before = mass_center(self.model, self.data)
         self.do_simulation(action, self.frame_skip)
         xy_position_after = mass_center(self.model, self.data)
@@ -155,9 +203,6 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
 
         joint_idx = [7, 8, 9, 14, 20, 21, 22, 23, 28, 34]
         
-        joint_idx = [15, 16, 20, 29, 30, 34]
-        joint_idx = [7, 8, 9, 14, 20, 21, 22, 23, 28, 34]
-        
         # if frameskip = 5, we don't need to multiply 6
         ref_qpos, ref_qvel = self.ref_trajectory.state(self.timestamp * self.frame_skip)
         ref_pelvis_pos = ref_qpos[0:3]
@@ -171,47 +216,32 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
         ref_mpos, ref_mvel, ref_torque = self.ref_trajectory.action(self.timestamp * self.frame_skip)
 
         # the following imitation reward design is from Zhaoming's 2023 paper https://zhaomingxie.github.io/projects/Opt-Mimic/opt-mimic.pdf
+        # sigmas = [0.05, 0.05, 0.3, 0.35, 0.3]
         sigmas = [1, 1, 1, 1, 1]
         reward_weights = [0.3, 0.3, 0.2, 0.1, 0.1] 
 
         # reward for pelvis position difference
-        r_0 = np.exp(- (np.linalg.norm(ref_pelvis_pos - current_pelvis_pos, ord=2) )/ (2 * sigmas[0] ) ) * 1e1
-        r_0 = np.exp(- (np.linalg.norm(ref_pelvis_pos - current_pelvis_pos, ord=2) ** 2 )/ (2 * sigmas[0]) )
-        r_0 = np.exp(- (np.linalg.norm(ref_pelvis_pos - current_pelvis_pos, ord=2) )/ (2 * sigmas[0] ) ) * 1e1
+        r_0 = np.exp(- (np.linalg.norm(ref_pelvis_pos - current_pelvis_pos, ord=2) **2 )/ (2 * sigmas[0] **2 ) ) 
         # reward for pelvis orientation difference
-        r_1 = np.exp(- (np.linalg.norm(ref_pelvis_ori - current_pelvis_ori, ord=2) ) / (2 * sigmas[1] ) ) * 1e1
-        r_1 = np.exp(- (np.linalg.norm(ref_pelvis_ori - current_pelvis_ori, ord=2) ** 2) / (2 * sigmas[1]) )
-        r_1 = np.exp(- (np.linalg.norm(ref_pelvis_ori - current_pelvis_ori, ord=2) ) / (2 * sigmas[1] ) ) * 1e1
+        r_1 = np.exp(- (np.linalg.norm(ref_pelvis_ori - current_pelvis_ori, ord=2) **2) / (2 * sigmas[1] **2 ) ) 
         # reward for joint position difference
-        r_2 = np.exp(-(np.linalg.norm(ref_joint_pos - current_joint_pos, ord=2) ) / (2 * sigmas[2] ) ) * 1e1
-        r_2 = np.exp(-(np.linalg.norm(ref_joint_pos - current_joint_pos, ord=2) ** 2) / (2 * sigmas[2]) )
-        r_2 = np.exp(-(np.linalg.norm(ref_joint_pos - current_joint_pos, ord=2) ) / (2 * sigmas[2] ) ) * 1e1
+        r_2 = np.exp(-(np.linalg.norm(ref_joint_pos - current_joint_pos, ord=2) **2) / (2 * sigmas[2] **2 ) ) 
         # reward for action difference
-        r_3 = np.exp(-(np.linalg.norm(ref_torque - action, ord=2) ) / (2 * sigmas[3] ) ) * 1e1
-        r_3 = np.exp(-(np.linalg.norm(ref_torque - action, ord=2) ** 2) / (2 * sigmas[3]) )
-        r_3 = np.exp(-(np.linalg.norm(ref_torque - action, ord=2) ) / (2 * sigmas[3] ) ) * 1e1
+        r_3 = np.exp(-(np.linalg.norm(ref_torque - action, ord=2) ) / (2 * sigmas[3] ) ) 
         # reward for maximum action difference
         current_max_action = np.max(np.abs(action))
         ref_max_action = np.max(np.abs(ref_torque))
-        r_4 = np.exp(-(np.abs(ref_max_action - current_max_action) ) / (2 * sigmas[4]) ) * 1e1
+        r_4 = np.exp(-(np.abs(ref_max_action - current_max_action) **2) / (2 * sigmas[4] **2) ) 
 
         r_5 = np.exp(-(np.linalg.norm(ref_qvel - self.data.qvel, ord=2) ) / (2 * 1 ) ) * 1e1 # + np.exp(-(np.linalg.norm(ref_qpos[:-1] - self.data.qpos))) * 1e1
-        r_4 = np.exp(-(np.abs(ref_max_action - current_max_action) ** 2) / (2 * sigmas[4]) )
-        r_4 = np.exp(-(np.abs(ref_max_action - current_max_action) ) / (2 * sigmas[4]) ) * 1e1
 
-        r_5 = np.exp(-(np.linalg.norm(ref_qvel - self.data.qvel, ord=2) ) / (2 * 1 ) ) * 1e1 # + np.exp(-(np.linalg.norm(ref_qpos[:-1] - self.data.qpos))) * 1e1
-
-        total_reward = reward_weights[0] * r_0 + reward_weights[1] * r_1 + reward_weights[2] * r_2 + reward_weights[3] * r_3 + reward_weights[4] * r_4 #+ 0.3 * r_5
-        # total_reward = -np.linalg.norm(self.data.qpos - ref_qpos[:-1])-np.linalg.norm(action-ref_torque)
-        total_reward = reward_weights[0] * r_0 + reward_weights[1] * r_1 + reward_weights[2] * r_2 + reward_weights[3] * r_3 + reward_weights[4] * r_4
         total_reward = reward_weights[0] * r_0 + reward_weights[1] * r_1 + reward_weights[2] * r_2 + reward_weights[3] * r_3 + reward_weights[4] * r_4 #+ 0.3 * r_5
         # total_reward = -np.linalg.norm(self.data.qpos - ref_qpos[:-1])-np.linalg.norm(action-ref_torque)
+        total_reward = np.exp(-np.linalg.norm(self.data.qpos - ref_qpos)) + np.exp(-np.linalg.norm(action-ref_torque))
         
         observation = self._get_obs()
         reward = total_reward 
-        # reward = forward_reward + healthy_reward - ctrl_cost
-        reward = total_reward + forward_reward + healthy_reward - ctrl_cost
-        reward = total_reward 
+        # reward = total_reward + forward_reward + healthy_reward - ctrl_cost
         # reward = forward_reward + healthy_reward - ctrl_cost
         terminated = self.terminated
         info = {
@@ -232,11 +262,13 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
         # print(ref_qpos[:3], current_pelvis_pos[:3])
         # print(reward)
         # if terminated:
-        #     print('final x pos', xy_position_after[0])
-        # print(ref_qpos[:3], current_pelvis_pos[:3])
-        # print(reward)
-        # if terminated:
-        #     print('final x pos', xy_position_after[0])
+            # exit()
+            # print(f'final x pos {xy_position_after[0]:.2f}, {ref_pelvis_pos[0]:.2f}, {current_pelvis_pos[0]:.2f}')
+        # print(f'{r_0:.2e}, {r_1:.2e}, {r_2:.2e}, {r_3:.2e}, {r_4:.2e}, {r_5:.2e}')
+        # print(f'{self.data.qpos[:3]}, {ref_pelvis_pos}')
+        # import time
+        # time.sleep(0.01)
+
         return observation, reward, terminated, False, info
 
     def reset_model(self):
@@ -250,6 +282,7 @@ class CassieEnv(MujocoEnv, utils.EzPickle):
             low=noise_low, high=noise_high, size=self.model.nv
         )
         self.set_state(qpos, qvel)
+        self.timestamp = 0
 
         observation = self._get_obs()
         return observation
diff --git a/mj_cassie.xml b/mj_cassie.xml
index f07483e..64cb7d1 100644
--- a/mj_cassie.xml
+++ b/mj_cassie.xml
@@ -1,7 +1,7 @@
 <mujoco model="cassie">
   <compiler eulerseq="zyx" meshdir="assets/mj_cassie" texturedir="./assets/mj_cassie" autolimits="true"/>
 
-  <option timestep="0.0005" solver="PGS" integrator="RK4" iterations="50"  />
+  <option timestep="0.0005" solver="PGS" integrator="RK4" iterations="50" gravity="0 0 -9.806" />
   <!-- Timestep is set to 0.0005 because the standard Cassie controller runs at 2 kHz -->
   <!-- Larger values still have stable dynamics -->
 
diff --git a/old_cassie/cassie_env/cassieRLEnv.py b/old_cassie/cassie_env/cassieRLEnv.py
index 7599799..84de004 100644
--- a/old_cassie/cassie_env/cassieRLEnv.py
+++ b/old_cassie/cassie_env/cassieRLEnv.py
@@ -218,7 +218,7 @@ class cassieRLEnv:
 
 class cassieRLEnvDelay(cassieRLEnv):
 	def __init__(self,visual):
-		self.model = "old_cassie/cassie_m/model/0cassie.xml"
+		self.model = "mj_cassie.xml"
 		self.sim = CassieSim(self.model)
 		self.visual = visual
 		if self.visual:
diff --git a/oldcassie.py b/oldcassie.py
index 35e7ff5..2560c18 100644
--- a/oldcassie.py
+++ b/oldcassie.py
@@ -52,7 +52,7 @@ class OldCassieMirrorEnv(gym.Env, utils.EzPickle):
                 low=-np.inf, high=np.inf, shape=(self._get_obs().shape[0],), dtype=np.float64
             )
         
-        self.action_space = Box(low=-np.inf, high=np.inf, shape=(10,), dtype=np.float64)
+        self.action_space = Box(low=-1, high=1, shape=(10,), dtype=np.float32)
 
     def reset(self, seed=None, options=None):
         self.env.reset()
diff --git a/testing.py b/testing.py
index 5f9424a..c138d9b 100644
--- a/testing.py
+++ b/testing.py
@@ -21,7 +21,7 @@ register(id='Digit-v1',
 		autoreset=True,)
 
 register(id='Cassie-v1',
-		entry_point='cassie:CassieEnv',
+		entry_point='mj_cassie:CassieEnv',
 		max_episode_steps=1000,
 		autoreset=True,)
 
@@ -55,7 +55,6 @@ def load_best_and_visualize():
                 replay_buffer=best_irl_model.replay_buffer,
                 log_interval=1,)
 	
-	
 def visualize_reference_traj():
 	env = make_vec_env("CassieViz-v1", n_envs=1, env_kwargs={'exclude_current_positions_from_observation': False, 'render_mode': 'human'})
 	model = SAC("MlpPolicy",
@@ -67,15 +66,19 @@ def visualize_reference_traj():
 def train_model():
 	config = {
 		"policy_type": "MlpPolicy",
-		"total_timesteps": int(1e7),
+		"total_timesteps": int(3e6),
 		"env_id": "Cassie-v1",
 		"progress_bar": True,
 		"verbose": 1,
 		"learning_rate": 3e-4,
 		"n_envs": 8,
+		"ent_coef": 0.1,
+		"gamma": 0.99,
+		"batch_size": 256,
+
 	}
 	run = wandb.init(
-		project="sb3",
+		project="New Cassie Testing",
 		config=config,
 		sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics
 		monitor_gym=True,  # auto-upload the videos of agents playing the game
@@ -85,20 +88,26 @@ def train_model():
 			model_save_path=f"models/{run.id}",
 			model_save_freq=10000,
 			gradient_save_freq=10000,
-			verbose=2,
+			verbose=1,
 		)
 	env = make_vec_env(config['env_id'], n_envs=config['n_envs'],)
 	# Separate evaluation env
 	eval_env = make_vec_env(config['env_id'], n_envs=1,)
 	# Use deterministic actions for evaluation
-	eval_callback = EvalCallback(eval_env, best_model_save_path="./logs/",
-									log_path="./logs/", eval_freq=10000,
-									deterministic=True, render=False)
+	eval_callback = EvalCallback(
+								eval_env, 
+								best_model_save_path="./logs/",
+				  				log_path="./logs/", eval_freq=5000,
+				  				deterministic=True, 
+								render=False
+								)
+	
 	callback_list = CallbackList([eval_callback, wandbcallback])
 	# Init model
 	model = SAC("MlpPolicy",
 				env,
 				verbose=config["verbose"],
+				ent_coef=config["ent_coef"],
 				learning_rate=config['learning_rate'],)
 	
 	model.learn(
@@ -110,11 +119,11 @@ def train_model():
 	run.finish()
 
 if __name__ == "__main__":
-	train = False
+	train = True
 	if train:
 		train_model()
 	else:
 		# load_best_and_visualize()
-		# visualize_reference_traj()
+		visualize_reference_traj()
 		# visualize_init_stance()
-		
\ No newline at end of file
+		pass
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index aa69826..be08f46 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20230702_212624-72ii2jgj
\ No newline at end of file
+run-20230703_193635-8uav8prq
\ No newline at end of file
