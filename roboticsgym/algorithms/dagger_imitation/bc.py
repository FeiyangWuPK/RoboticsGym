"""Behavioural Cloning (BC).

Trains policy by applying supervised learning to a fixed dataset of (observation,
action) pairs generated by some expert demonstrator.
"""

import dataclasses

import gymnasium as gym
import numpy as np

from tqdm import tqdm
from typing import Any, ClassVar, Dict, Iterable, List, Optional, Tuple, Type, TypeVar, Union, Mapping
from stable_baselines3.common import torch_layers, utils, vec_env

from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule
from stable_baselines3.common.policies import BasePolicy, ActorCriticPolicy

from stable_baselines3.common.base_class import BaseAlgorithm



import torch
from torch.utils.data import Dataset, DataLoader

from .networks import FeedForward32Policy


@dataclasses.dataclass(frozen=True)
class BCTrainingMetrics:
    """Container for the different components of behavior cloning loss."""
    neglogp: torch.Tensor
    entropy: torch.Tensor
    ent_loss: torch.Tensor  # set to 0 if entropy is None
    prob_true_act: torch.Tensor
    l2_norm: torch.Tensor
    l2_loss: torch.Tensor
    loss: torch.Tensor


@dataclasses.dataclass(frozen=True)
class BehaviorCloningLossCalculator:
    """Functor to compute the loss used in Behavior Cloning."""

    ent_weight: float
    l2_weight: float

    def __call__(
        self,
        policy: BasePolicy,
        obs: torch.Tensor,
        acts: torch.Tensor,
    ) -> BCTrainingMetrics:
        """Calculate the supervised learning loss used to train the behavioral clone.

        Args:
            policy: The actor-critic policy whose loss is being computed.
            obs: The observations seen by the expert.
            acts: The actions taken by the expert.

        Returns:
            A BCTrainingMetrics object with the loss and all the components it
            consists of.
        """
        # policy.evaluate_actions's type signatures are incorrect.
        # See https://github.com/DLR-RM/stable-baselines3/issues/1679


        (_, log_prob, entropy) = policy.evaluate_actions(obs, acts)

        prob_true_act = torch.exp(log_prob).mean()
        log_prob = log_prob.mean()
        entropy = entropy.mean() if entropy is not None else None

        l2_norms = [torch.sum(torch.square(w)) for w in policy.parameters()]
        l2_norm = sum(l2_norms) / 2  # divide by 2 to cancel with gradient of square
        # sum of list defaults to float(0) if len == 0.
        assert isinstance(l2_norm, torch.Tensor)

        ent_loss = -self.ent_weight * (entropy if entropy is not None else torch.zeros(1))
        neglogp = -log_prob
        l2_loss = self.l2_weight * l2_norm
        loss = neglogp + ent_loss + l2_loss

        return BCTrainingMetrics(
            neglogp=neglogp,
            entropy=entropy,
            ent_loss=ent_loss,
            prob_true_act=prob_true_act,
            l2_norm=l2_norm,
            l2_loss=l2_loss,
            loss=loss,
        )


class BC(BaseAlgorithm):
    """Behavioral cloning (BC).

    Recovers a policy via supervised learning from observation-action pairs.
    """
    def __init__(
        self,
        *,
        env: Union[GymEnv, str, None],
        policy: Union[str, Type[BasePolicy]] = None,
        learning_rate: Union[float, Schedule] = None,
        policy_kwargs: Optional[Dict[str, Any]] = None,
        device: Union[torch.device, str] = "auto",

        rng: np.random.Generator,

        batch_size: int = 32,
        n_epochs: int = 4,
        optimizer_cls: Type[torch.optim.Optimizer] = torch.optim.Adam,
        optimizer_kwargs: Optional[Mapping[str, Any]] = None,
        ent_weight: float = 1e-3,
        l2_weight: float = 0.0,
    ):
        print("BC__init__()")

        self._batch_size = batch_size
        self.n_epochs = n_epochs
      
        self.action_space = env.action_space
        self.observation_space = env.observation_space

        if isinstance(env.observation_space, gym.spaces.Dict):
            self.observation_space = self.observation_space['observation']

        print("self.observation_space",  type(self.observation_space))
        print("self.action_space",type(self.action_space))

        if policy is None:
            extractor = (
                torch_layers.CombinedExtractor
                if False
                else
                torch_layers.FlattenExtractor
            )
            policy = FeedForward32Policy(
                observation_space=self.observation_space,
                action_space=self.action_space,
                # Set lr_schedule to max value to force error if policy.optimizer
                # is used by mistake (should use self.optimizer instead).
                lr_schedule=lambda _: torch.finfo(torch.float32).max,
                features_extractor_class=extractor,
            )


        self._policy = policy.to(utils.get_device(device))
        assert self.policy.observation_space == self.observation_space
        assert self.policy.action_space == self.action_space

        if optimizer_kwargs:
            if "weight_decay" in optimizer_kwargs:
                raise ValueError("Use the parameter l2_weight instead of weight_decay.")
        optimizer_kwargs = optimizer_kwargs or {}

        self.optimizer = optimizer_cls(self.policy.parameters(), **optimizer_kwargs)
        self.loss_calculator = BehaviorCloningLossCalculator(ent_weight, l2_weight)

        self.rng = rng
        self.sample_so_far = 0

        super().__init__(
            policy=self.policy,
            env=env,
            learning_rate=self.optimizer.param_groups[0]['lr'],
            device=device,
            support_multi_env=True,
        )

        print("device", self.device)
        print("batch", self.batch_size)


    def set_demonstrations(self, demonstrations: DataLoader) -> None:
            self.demonstrations = demonstrations

    def save(self,path):
        self.policy.save("save_bc")

    def learn(
        self,
        *,
        total_timesteps: int = None,
        callback: MaybeCallback = None,
        log_interval: int = 500,
        tb_log_name: str = "run",
        reset_num_timesteps: bool = True,
        progress_bar: bool = False,
    ):
        """Train with supervised learning for some number of epochs.

        Here an 'epoch' is just a complete pass through the expert data loader,
        as set by `self.set_expert_data_loader()`. Note, that when you specify
        `n_batches` smaller than the number of batches in an epoch, the `on_epoch_end`
        callback will never be called.

        Args:
            n_epochs: Number of complete passes made through expert data before ending
                training. Provide exactly one of `n_epochs` and `n_batches`.
            n_batches: Number of batches loaded from dataset before ending training.
                Provide exactly one of `n_epochs` and `n_batches`.
            log_interval: Log stats after every log_interval batches.
            log_rollouts_venv: If not None, then this VecEnv (whose observation and
                actions spaces must match `self.observation_space` and
                `self.action_space`) is used to generate rollout stats, including
                average return and average episode lengtorch. If None, then no rollouts
                are generated.
            log_rollouts_n_episodes: Number of rollouts to generate when calculating
                rollout stats. Non-positive number disables rollouts.
            progress_bar: If True, then show a progress bar during training.
            reset_tensorboard: If True, then start plotting to Tensorboard from x=0
                even if `.train()` logged to Tensorboard previously. Has no practical
                effect if `.train()` is being called for the first time.
        """
        assert self.demonstrations is not None

        # super().learn(
        #     total_timesteps=total_timesteps,
        #     callback=callback,
        #     log_interval=log_interval,
        #     tb_log_name=tb_log_name,
        #     reset_num_timesteps=reset_num_timesteps,
        #     progress_bar=progress_bar,
        # )


        
        print_sample_count = True
        for epoch in range(self.n_epochs):
            print("Epoch ", epoch)
            print("Sample_so_far", self.sample_so_far)
            for i, batch in enumerate(tqdm(self.demonstrations, desc='Training '), 0):

                obs, acts = batch
                obs  = obs.to(self.device)
                acts = acts.to(self.device)

                self.sample_so_far += len(obs)
                if(print_sample_count):
                    print("sample_so_far",self.sample_so_far)
                    print_sample_count = False

                
                training_metrics = self.loss_calculator(self.policy, obs, acts)

                loss = training_metrics.loss

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                # if batch_count % 100 == 0:
                #     print("batch:", batch_count, "loss:", loss.item())


    def predict(self,
        observation: Union[np.ndarray, Dict[str, np.ndarray]],
        state: Optional[Tuple[np.ndarray, ...]] = None,
        episode_start: Optional[np.ndarray] = None,
        deterministic: bool = False,
    ) -> Tuple[np.ndarray, Optional[Tuple[np.ndarray, ...]]]:

        return self.policy.predict(observation, state, episode_start, deterministic)
    
        

    @property
    def policy(self) -> BasePolicy:
        return self._policy
    
    @property
    def batch_size(self) -> int:
        return self._batch_size


    def _setup_model():
        return



